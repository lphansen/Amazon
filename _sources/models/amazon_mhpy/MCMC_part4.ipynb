{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a753ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Import Required Packages\n",
    "# ========================\n",
    "import os, sys\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import casadi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "\n",
    "sys.path.append(os.path.abspath(\"mcmc\"))\n",
    "from mcmc_sampling import create_hmc_sampler\n",
    "\n",
    "# Local Debugging flag; remove when all tested\n",
    "_DEBUG = False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b63e61-abd7-416d-9873-591e41862bd8",
   "metadata": {},
   "source": [
    "We investigate a static formulation of robustness to parameter uncertainty.  For each site, we consider the parameter pair $\\beta^i = (\\gamma^i, \\theta^i)$ for $i=1,2,...,I.$ Let\n",
    "$\\mathbf{\\beta}$ denote full parameter vector including all sites and hence of dimension $2 \\times I.$  \n",
    "\n",
    "Our planner is uncertain about these parameter values and instead has baseline \n",
    "probability distribution $\\pi.$  In addition, this planner is uncertain about what  distribution to use and instead  thinks of $\\pi$ as a rough approximation.  We address this uncertainty by \n",
    "introducing ambiguity about the parameter distribution.  \n",
    "\n",
    "Let $d$ be the vector of decisions and $f( \\mathbf{d},\\mathbf{\\beta})$ for the resulting value  given the unknown parameter $\\mathbf{\\beta}.$  We use a divergence measure to capture  ambiguity about the parameter distribution.  For $\\int g( \\mathbf{\\beta}) d\\pi( \\mathbf{\\beta}) = 1,$ the relative entropy (or Kullback-Leibler) divergence \n",
    "\n",
    "$$\n",
    "\\int_{\\mathcal B} \\log g(\\mathbf{\\beta}) g( \\mathbf{\\beta}) d \\pi(\\mathbf{ \\beta}) \\ge 0,  \n",
    "$$\n",
    "\n",
    "is a commonly used measure of divergence between a probability $g(\\beta)  d\\pi(  \\beta)$ and the baseline $d\\pi(\\beta).$\n",
    " To produce optimal controls that are robust to the parameter uncertainty, solve\n",
    "\n",
    "$$\n",
    "\\max_{ d} \\min_{g, \\int g d\\pi = 1} \\int_{\\mathcal B} f(\\mathbf{ d},  \\mathbf{ \\beta} )   g(\\mathbf{ \\beta}) d\\pi( \\mathbf{\\beta}) \n",
    "+ \\xi \\int \\log g( \\mathbf{\\beta}) g(\\mathbf{ \\beta} ) d\\pi( \\mathbf{\\beta}) \n",
    "$$\n",
    "where $\\xi > 0$ is penalty parameter.\\footnote{Alternatively, we can think of $\\xi$ a as Lagrange multiplier on a relative entropy divergence constraint.}\n",
    "\\end{problem}\n",
    "\n",
    "We implement a full commitment to the baseline distribution by  making $\\xi$  arbitrarily large.  More modest settings capture a concern for robustness.\n",
    "\n",
    "One nice feature of using relative entropy divergence to explore distributional sensitivity is that the minimization problem has a quasi-analytical solution:\n",
    "\n",
    "$$\n",
    "-\\xi \\log \\int_{\\mathcal B} \\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{ d},  \\mathbf{ \\beta} )  \\right]  d\\pi( \\beta) = \\min_{g, \\int g d\\pi = 1} \\int_{\\mathcal B} f(\\mathbf{ d},  \\mathbf{ \\beta} )   g(\\mathbf{ \\beta}) d\\pi(\\mathbf {\\beta}) \n",
    "+ \\xi \\int \\log g(\\mathbf{ \\beta}) g( \\mathbf{\\beta}) d\\pi( \\mathbf{\\beta})\n",
    "$$\n",
    "\n",
    "where the minimizing $g$ given by:\n",
    "\n",
    "$$\n",
    "g^* = \\frac {{\\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{d},  \\mathbf{\\beta} )\\right]}}\n",
    "{{\\int_{\\mathcal B} \\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{d}, \\tilde  {\\mathbf{\\beta}})\\right]d\\pi( \\tilde {\\mathbf{\\beta}} )}}\n",
    "$$\n",
    "\n",
    "This tilts the distribution towards smaller objectives for each given decision $\\mathbf{d}$. The candidate solution presumes that: \n",
    "\n",
    "$$\n",
    "\\int_{\\mathcal B} \\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{d},  \\mathbf{\\beta})\\right]d\\pi(\\mathbf{\\beta} ) < \\infty\n",
    "$$\n",
    "\n",
    "which implicitly limits how fat the tail can be for the distribution $\\pi$.\n",
    "\n",
    "**Remark:**\n",
    "\n",
    "The formula on the left-side of equation \\eqref{{quasi}} is a special case of a smooth ambiguity objective, first suggested by \\cite{{KlibanoffMarinacciMukerji:2005}}.  They deduced a rationale for an ambiguity adjustment represented using  a concave function distinct from the one used for expressing risk aversion.  Thus the negative exponential in equation \\eqref{{quasi}} is such a concave function.   The logarithmic adjustment converts this to a certainty equivalent. Their axiomatic motivation is quite different from the distributional robustness that is of interest to us, however.\n",
    "\n",
    "For conceptual reasons, we also switch the order of the maximization and minimization.  Under quite general conditions, we can invoke the min-max theorem:\n",
    "\n",
    "$$\n",
    "\\begin{{problem}} \\label{{prob:reverse}}\n",
    "\\min_{g, \\int g d\\pi = 1} \\max_{\\mathbf{d}}  \\int_{\\mathcal B} f(\\mathbf{d},  \\mathbf{\\beta} )   g(\\mathbf{\\beta}) d\\pi(\\mathbf{\\beta}) \n",
    "+ \\xi \\int \\log g(\\mathbf{\\beta}) g(\\mathbf{\\beta}) d\\pi(\\mathbf{\\beta}) \n",
    "\\text{{where }} \\xi > 0 \\text{{ is penalty parameter.}}\n",
    "\\end{{problem}}\n",
    "$$\n",
    "\n",
    "Consider the inner maximization problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\mathbf{d}}  \\int_{\\mathcal B} f(\\mathbf{d},  \\mathbf{\\beta} )   g(\\mathbf{\\beta}) d\\pi(\\mathbf{\\beta})\n",
    "$$\n",
    "\n",
    "where we are free to drop the relative entropy penalty as it does not depend on the decision $\\mathbf{d}$.  Provided that this problem has a solution for the outer $g$ minimization, then the planner is maximizing against this particular (penalized) \"worst case probability.\"\n",
    "\n",
    "This computation is of interest as a way to interpret the consequences of any given choice of the penalty parameter $\\xi$.  In practice, we find it revealing to explore alternative choices of $\\xi$ and deduce their implications for the implied worst case probabilities.\\footnote{{This follows a common practice for robust Bayesian methods.}}\n",
    "\n",
    "For solving the robust problem numerically, we take an iterative approach, also supported by the min-max theorem.  \n",
    "\n",
    "Specifically, we proceed as follows:   \n",
    "\n",
    "1. Given a $g$, we solve the maximization problem for a candidate $\\mathbf{d}.$  We ignore the relative entropy penalty term in this solution.  \n",
    "2. For a given $\\mathbf{d}$, we solve the minimization problem with the relative entropy term to obtain a new  candidate for $g$.\n",
    "3. We repeat the steps until we achieve  convergence. \n",
    "\n",
    "For step 2, we use a Hamiltonian simulation approach along with the quasi-analytical solution in computing equation \\eqref{{min_solution}}.  A numerical method is necessary because of the denominator term, and Markov simulation gives us one way to explore a parameter space of potentially large dimension.  Very similar to how a Metropolis-Hastings algorithm can be used  to compute a Bayesian posterior in terms of a prior and a likelihood, we calculate the exponentially tilted solution using $d\\pi(\\mathbf{\\beta})$ and $\\exp\\left[-{\\frac 1 \\xi} f(\\mathbf{d}, \\mathbf{\\beta})\\right].$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee42f0-17fd-489b-a869-325f2ae329d0",
   "metadata": {},
   "source": [
    "# Multivariate Normal Log Probability\n",
    "\n",
    "The function `multivariate_normal_log_probability(z, mean, cov)` calculates and returns the log-probability of a (unnormalized) multivariate normal distribution. The parameters of the function are:\n",
    "\n",
    "- `z` : The value of the multivariate normal random variable.\n",
    "- `mean` : A one-dimensional array or list that represents the mean of the normal distribution.\n",
    "- `cov` : A two-dimensional array that represents the covariance matrix.\n",
    "\n",
    "This function works by first converting all input parameters into NumPy arrays of floating point numbers. It also flattens the `z` and `mean` arrays to ensure they're one-dimensional.\n",
    "\n",
    "The function then performs assertions to verify that the input shapes and types are correct. The size of the `mean` array should be equal to the size of `z`. If `mean` has more than one element, the shape of `cov` should be a square matrix (z.size x z.size). If `mean` has only one element, `cov` should be a single number, which is then reshaped into a 1x1 matrix.\n",
    "\n",
    "The log-probability of the (unnormalized) multivariate normal distribution is then calculated as follows:\n",
    "\n",
    "1. Calculate the deviation `dev` from the `mean`.\n",
    "2. Scale the deviation `scld_dev` by the inverse of the covariance matrix.\n",
    "3. Compute the log probability `log_prob` as -0.5 times the dot product of the deviation and the scaled deviation.\n",
    "\n",
    "The function finally returns the computed log probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a650f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal_log_probability(z, mean, cov):\n",
    "    \"\"\"\n",
    "    Evaluate and return the log-probability of a (unnormalized) multivariate normal distribution.\n",
    "\n",
    "    :param z: value of the multivariate normal random variable\n",
    "    :param mean: (1d array/list); mean of the normal distribution\n",
    "    :param cov: (2D array) covariance matrix\n",
    "\n",
    "    :returns: log-probability of a (unnormalized) multivariate normal distribution\n",
    "    \"\"\"\n",
    "    z    = np.asarray(z, dtype=float).flatten()\n",
    "    mean = np.asarray(z, dtype=float).flatten()\n",
    "    cov = np.asarray(cov, dtype=float).squeeze()\n",
    "\n",
    "    # Assert/Check input types/shapes\n",
    "    assert mean.size == z.size, \"mismatch between variable and mean sizes/shapes\"\n",
    "    if mean.size > 1:\n",
    "        assert cov.shape == (z.size,z.size), \"mismatch between variable and covariances `cov` shape\"\n",
    "\n",
    "    elif mean.size == 1:\n",
    "        assert cov.size == 1, \"mismatch between variable and covariances `cov` shape\"\n",
    "        cov = np.reshape(cov, (1, 1))\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid covariance matrix shape!\")\n",
    "        raise AssertionError\n",
    "\n",
    "    # Evaluate the log-probability\n",
    "    dev      = z - mean\n",
    "    scld_dev = np.linalg.inv(cov).dot(dev)\n",
    "    log_prob = - 0.5 * np.dot(dev, scld_dev)\n",
    "\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc552a3c-8da2-44f3-b6a6-5609a3a25451",
   "metadata": {},
   "source": [
    "# Load Site Data Function\n",
    "\n",
    "The function `load_site_data(site_num, norm_fac=1.0)` is designed to load and preprocess site-specific data. The function takes two parameters:\n",
    "\n",
    "- `site_num` : An integer representing the site number.\n",
    "- `norm_fac` : A float representing the normalization factor. Its default value is 1.0.\n",
    "\n",
    "The function starts by reading a CSV file specific to the site number passed as the `site_num` parameter. The file should be named as \"calibration_{n}SitesModel.csv\", where `{n}` is the site number.\n",
    "\n",
    "The function then extracts information from several columns in the data frame. The columns are named using the site number and the corresponding information type (e.g., `z_2017_{n}Sites`, `zbar_2017_{n}Sites`, etc.).\n",
    "\n",
    "The extracted information includes:\n",
    "\n",
    "- `z_2017` : Data specific to the site from 2017.\n",
    "- `zbar_2017` : Mean data specific to the site from 2017.\n",
    "- `gamma` : Gamma value for the site.\n",
    "- `gammaSD` : Standard deviation of the gamma value for the site.\n",
    "- `forestArea_2017_ha` : Forest area for the site in 2017 (in hectares).\n",
    "- `theta` : Theta value for the site.\n",
    "\n",
    "After extracting the data, the function normalizes `zbar_2017` and `z_2017` using the normalization factor `norm_fac`.\n",
    "\n",
    "Finally, the function returns a tuple containing the normalized `zbar_2017`, `gamma`, `gammaSD`, `z_2017`, `forestArea_2017_ha`, and `theta` data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23697e0d-7eda-405b-9dcd-0b7f5449df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_num = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f5da72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_site_data(site_num, \n",
    "                   norm_fac=1.0,\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    Load site data\n",
    "\n",
    "    :returns:\n",
    "        -\n",
    "    \"\"\"\n",
    "    # Read data file\n",
    "    n=site_num\n",
    "    file=f\"data/calibration_{n}SitesModel.csv\"\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    \n",
    "    # Extract information\n",
    "    z_2017             = df[f'z_2017_{n}Sites'].to_numpy()\n",
    "    zbar_2017          = df[f'zbar_2017_{n}Sites'].to_numpy()\n",
    "    gamma              = df[f'gamma_{n}Sites'].to_numpy()\n",
    "    gammaSD            = df[f'gammaSD_{n}Sites'].to_numpy()\n",
    "    forestArea_2017_ha = df[f'forestArea_2017_ha_{n}Sites'].to_numpy()\n",
    "    theta              = df[f'theta_{n}Sites'].to_numpy()\n",
    "\n",
    "    # Normalize Z data\n",
    "    zbar_2017 /= norm_fac\n",
    "    z_2017    /= norm_fac\n",
    "    \n",
    "    return (zbar_2017,\n",
    "            gamma,\n",
    "            gammaSD,\n",
    "            z_2017,\n",
    "            forestArea_2017_ha,\n",
    "            theta\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06da816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma [516.88557127 488.95668786 535.7933372  614.38476229 697.34085963\n",
      " 594.38440765 540.18969668 502.09011583 586.13789618 622.92870153\n",
      " 468.53543538 216.29945459 736.1126944  644.37719596 568.13204882\n",
      " 618.56342492 570.93769461 376.7589174  806.29456683 649.96115287\n",
      " 577.10631912 455.99483831 366.09394675 337.21852297 287.82630347]\n",
      "gammaSD [22.01380318 12.30650843 23.34479384 32.19887999 33.0552216  20.86507363\n",
      " 10.44879734  7.97210675 16.35650086 18.89356099  5.66673842  4.7426229\n",
      " 27.52295223 16.35653107 10.55158851 21.85866557 24.68789648  7.04434639\n",
      " 35.21752443 25.01272143 13.69821009  6.68635044  1.78628409 14.31382086\n",
      " 17.20154406]\n",
      "z_2017 [5.77092940e-07 5.58918518e-06 7.87674083e-04 9.64686174e-06\n",
      " 1.28634503e-04 1.62486537e-05 7.71270596e-05 4.13615376e-04\n",
      " 1.41563731e-03 2.97549040e-03 7.91141025e-03 3.11951500e-04\n",
      " 4.91859804e-04 5.48889538e-04 1.19367249e-03 1.41661798e-03\n",
      " 6.05514368e-03 6.74644202e-03 8.44104902e-05 2.44820053e-03\n",
      " 7.38590342e-03 8.45649924e-03 5.22554843e-03 8.60690505e-04\n",
      " 2.31193733e-03]\n",
      "gamma - gammaSD [494.87176808 476.65017943 512.44854336 582.1858823  664.28563803\n",
      " 573.51933402 529.74089934 494.11800908 569.78139532 604.03514054\n",
      " 462.86869696 211.55683169 708.58974217 628.02066489 557.58046031\n",
      " 596.70475936 546.24979813 369.71457101 771.0770424  624.94843144\n",
      " 563.40810902 449.30848787 364.30766265 322.90470211 270.62475941]\n",
      "zbar_2017 [0.00082784 0.00592396 0.0162693  0.00831711 0.01149272 0.00364199\n",
      " 0.02794187 0.02603153 0.02649412 0.02588343 0.01898131 0.00098645\n",
      " 0.02398373 0.02837916 0.02750545 0.0280807  0.0275304  0.00982846\n",
      " 0.00437791 0.00834653 0.0212187  0.02077097 0.01650749 0.00157716\n",
      " 0.00345833]\n"
     ]
    }
   ],
   "source": [
    "# Verbose Data check (eyeball check!)\n",
    "zbar_2017, gamma, gammaSD, \\\n",
    "            z_2017, forestArea_2017_ha, theta = load_site_data(site_num,norm_fac=1e+9)\n",
    "print(\"gamma\", gamma)\n",
    "print(\"gammaSD\", gammaSD)\n",
    "print(\"z_2017\", z_2017)\n",
    "print(\"gamma - gammaSD\", gamma - gammaSD)\n",
    "print(\"zbar_2017\", zbar_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa925738-4099-4957-b333-5bce5b06bcf8",
   "metadata": {},
   "source": [
    "# Log Density Function\n",
    "\n",
    "The function `log_density_function()` evaluates the log-density of the objective or posterior distribution. This function is used within an optimization loop, with some parameters updated in each iteration of the loop.\n",
    "\n",
    "The function takes a large number of parameters, some of which include:\n",
    "\n",
    "- `gamma_val` : Gamma values.\n",
    "- `gamma_vals_mean` : Mean of gamma values.\n",
    "- `theta_vals` : Theta values.\n",
    "- `site_precisions` : Precision of site data.\n",
    "- `alpha` : Alpha values.\n",
    "- `sol` : Solution values from optimization.\n",
    "- And several other parameters.\n",
    "\n",
    "Inside the function, the `gamma_val` is flattened and a few initial computations are made. `X_zero` is calculated using `gamma_val`, `forestArea_2017_ha`, and `norm_fac`. `X_dym` is calculated using `alpha_p_Adym`, `X_zero`, `Bdym`, and `omega`.\n",
    "\n",
    "The function then modifies the solution of `X` (i.e., `sol.value(X)`) for further computation. This modification includes shifting and scaling operations on the `X` matrix.\n",
    "\n",
    "The function computes the log-density in three steps:\n",
    "\n",
    "- `term_1` is calculated as the negative sum of the product of `ds_vect`, `sol.value(Ua)`, and `zeta`, all divided by 2.\n",
    "- `term_2` is computed as the sum of the product of `ds_vect`, `pf`, and the difference between successive elements of `X_dym`.\n",
    "- `term_3` is calculated as the sum of the product of `ds_vect` and the sum of `z_shifted_X`.\n",
    "\n",
    "The objective value `obj_val` is the sum of `term_1`, `term_2`, and `term_3`.\n",
    "\n",
    "The function also computes `norm_log_prob` as the negative half of the dot product of `gamma_val_dev` and the dot product of `site_precisions` and `gamma_val_dev`.\n",
    "\n",
    "The log density value `log_density_val` is calculated as `-1.0 / xi * obj_val + norm_log_prob`.\n",
    "\n",
    "If the global `_DEBUG` flag is set, the function prints out the values of `term_1`, `term_2`, `term_3`, `obj_val`, `norm_log_prob`, and `log_density_val`.\n",
    "\n",
    "Finally, the function returns the `log_density_val`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "518ca24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_density_function(gamma_val,\n",
    "                         gamma_vals_mean,\n",
    "                         theta_vals,\n",
    "                   \n",
    "                         site_precisions,\n",
    "                         alpha,\n",
    "                         sol,\n",
    "                         X,\n",
    "                         Ua,\n",
    "                         Up,\n",
    "                         zbar_2017,\n",
    "                         forestArea_2017_ha,\n",
    "                         norm_fac,\n",
    "                         alpha_p_Adym,\n",
    "                         Bdym,\n",
    "                         leng,\n",
    "                         T,\n",
    "                         ds_vect,\n",
    "                         zeta,\n",
    "                         xi,\n",
    "                         kappa,\n",
    "                         pa,\n",
    "                         pf,\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Define a function to evaluate log-density of the objective/posterior distribution\n",
    "    Some of the input parameters are updated at each cycle of the outer loop (optimization loop),\n",
    "    and it becomes then easier/cheaper to udpate the function stamp and keep it separate here\n",
    "    \"\"\"\n",
    "    N          = X.shape[1] - 1\n",
    "    \n",
    "    gamma_val  = np.asarray(gamma_val).flatten()\n",
    "    gamma_size = gamma_val.size\n",
    "    x0_vals    = gamma_val.T.dot(forestArea_2017_ha) / norm_fac\n",
    "    X_zero     = np.sum(x0_vals) * np.ones(leng)\n",
    "    \n",
    "    \n",
    "    # shifted_X = zbar_2017 - sol.value(X)[0:gamma_size, :-1]\n",
    "    shifted_X  = sol.value(X)[0: gamma_size, :-1].copy()\n",
    "    for j in range(N): \n",
    "        shifted_X[:, j]  = zbar_2017 - shifted_X[:, j]\n",
    "    omega      = np.dot(gamma_val, alpha * shifted_X - sol.value(Up))\n",
    "    \n",
    "    X_dym      = np.zeros(T+1)\n",
    "    X_dym[0]   = np.sum(x0_vals)\n",
    "    X_dym[1: ] = alpha_p_Adym * X_zero  + np.dot(Bdym, omega.T)\n",
    "\n",
    "    z_shifted_X = sol.value(X)[0: gamma_size, :].copy()\n",
    "    scl = pa * theta_vals - pf * kappa\n",
    "    for j in range(N+1): z_shifted_X [:, j] *= scl\n",
    "    \n",
    "    term_1 = - casadi.sum2(np.reshape(ds_vect[0: T], (1, T)) * sol.value(Ua) * zeta / 2 )\n",
    "    term_2 = casadi.sum2(np.reshape(ds_vect[0: T], (1, T)) * pf * (X_dym[1: ] - X_dym[0: -1]))\n",
    "    term_3 = casadi.sum2(np.reshape(ds_vect, (1, N+1)) * casadi.sum1(z_shifted_X))\n",
    "    \n",
    "    obj_val = term_1 + term_2 + term_3\n",
    "    \n",
    "    gamma_val_dev   = gamma_val - gamma_vals_mean\n",
    "    norm_log_prob   =   - 0.5 * np.dot(gamma_val_dev,\n",
    "                                       site_precisions.dot(gamma_val_dev)\n",
    "                                       )\n",
    "    log_density_val = -1.0  / xi * obj_val + norm_log_prob\n",
    "\n",
    "    if _DEBUG:\n",
    "        print(\"Term 1: \", term_1)\n",
    "        print(\"Term 2: \", term_2)\n",
    "        print(\"Term 3: \", term_3)\n",
    "        print(\"obj_val: \", obj_val)\n",
    "        print(\"norm_log_prob\", norm_log_prob)\n",
    "        print(\"log_density_val\", log_density_val)\n",
    "\n",
    "    return log_density_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b8bc4-5700-43db-9cc5-347159ab7245",
   "metadata": {},
   "source": [
    "# Description of the `main` Function\n",
    "\n",
    "The `main` function is a complex piece of code that is designed to model and solve an optimization problem using Hamiltonian Monte Carlo (HMC) sampling and the CasADi framework for nonlinear optimization.\n",
    "\n",
    "## Parameters of the Function\n",
    "\n",
    "The function accepts numerous parameters, most of which have default values:\n",
    "\n",
    "- `norm_fac`, `delta_t`, `alpha`, `kappa`, `pf`, `pa`, `xi`, `zeta`: These seem to be configurations or settings for the model.\n",
    "\n",
    "- `max_iter`, `tol`, `T`, `N`: These are parameters that control the iterations in the optimization process.\n",
    "\n",
    "- `sample_size`, `mode_as_solution`, `final_sample_size`: These parameters relate to the sampling process for the Hamiltonian Monte Carlo method.\n",
    "\n",
    "## Core Function Logic\n",
    "\n",
    "The function begins by loading data related to the sites and performing some preliminary calculations on gamma values. It then calculates the mean and covariances from the site data. It sets up various data structures and matrices to hold information as it iterates through the main loop.\n",
    "\n",
    "The main loop is a convergence loop that continues until the maximum number of iterations is reached or the error is below a specified tolerance. In each loop, the function:\n",
    "\n",
    "- Updates the values of x0 and constructs matrices A and D using the current gamma values.\n",
    "\n",
    "- Defines the right-hand side of an equation to be solved by a solver. \n",
    "\n",
    "- Sets up and solves an optimization problem using the CasADi `Opti` class.\n",
    "\n",
    "- Generates samples using the Hamiltonian Monte Carlo method. It then updates the gamma values and computes an error metric for convergence.\n",
    "\n",
    "- If the convergence criterion is met, the function exits the loop, otherwise, it continues to the next iteration.\n",
    "\n",
    "The function concludes by sampling the final distribution densely and returning the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307e5340-9bd1-484e-be22-55c55a482cee",
   "metadata": {},
   "source": [
    "# Hamiltonian Monte Carlo\n",
    "\n",
    "Below is a descirption of the Hamiltonian Monte Carlo\n",
    "1. **Initialization**: Start with an initial guess for $\\beta$, typically denoted as $\\bar{\\beta}$. Simultaneously, generate a random value for $\\mu$ from its marginal distribution. \n",
    "\n",
    "2. **Symplectic Integration**: Use a discrete approximation method that preserves volume (like the Leapfrog method), also known as a symplectic integrator, to simulate the Hamiltonian dynamics. This process involves choosing a step size and running the simulation for a predetermined number of steps, typically around 20 but can be tailored based on the problem at hand. The output from this step is a new proposed point in the parameter space, denoted as $(\\beta^*,\\mu^*)$.\n",
    "\n",
    "3. **Metropolis Acceptance**: The Metropolis algorithm is then employed to decide whether to accept or reject the proposed point. The acceptance probability is given by $\\min\\{1, \\exp\\left(H(\\beta,\\mu)-H(\\beta^*,\\mu^*)\\right)\\}$, with discretization allowing for a probability less than 1.\n",
    "    - It is important to note that the volume-preservation property of the symplectic integrator guarantees that the Metropolis update leaves the canonical distribution for $(\\beta, \\mu)$ invariant.\n",
    "\n",
    "4. **Iteration**: Generate another random value for $\\mu'$ and repeat the steps 2 and 3 starting from the new point $(\\beta^*,\\mu')$. If the proposed point is rejected in the Metropolis step, we stick with the current $\\beta$ and repeat the process.\n",
    "\n",
    "5. **Decision Recalculation**: After each full iteration, the decision $d$ is recalculated.\n",
    "\n",
    "6. **Convergence**: The process from steps 2 to 5 is repeated until the average $\\beta$ value converges to a stable point.\n",
    "\n",
    "7. **Final Distribution**: Lastly, the HMC is run for a large number of iterations (e.g., 10,000) to produce the final distorted distribution. This comprehensive run allows for more robust estimation of the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a21fbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(\n",
    "    # Configurations/Settings\n",
    "    norm_fac          = 1e9,\n",
    "    delta_t           = 0.02,\n",
    "    alpha             = 0.045007414,\n",
    "    kappa             = 2.094215255,\n",
    "    pf                = 20.76,\n",
    "    pa                = 44.75,\n",
    "    xi                = 0.01,\n",
    "    zeta              = 1.66e-4*1e9,  # zeta := 1.66e-4*norm_fac  #\n",
    "    #\n",
    "    max_iter          = 200,\n",
    "    tol               = 0.01,\n",
    "    T                 = 200,\n",
    "    N                 = 200,\n",
    "    #\n",
    "    sample_size       = 1000,    # simulations before convergence (to evaluate the mean)\n",
    "    mode_as_solution  = False,   # If true, use the mode (point of high probability) as solution for gamma\n",
    "    final_sample_size = 100_00, # number of samples to collect after convergence\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Main function; putting things together\n",
    "\n",
    "    :param float tol: convergence tolerance\n",
    "    :param T:\n",
    "    :param N:\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Load sites' data\n",
    "    zbar_2017, gamma, gammaSD, \\\n",
    "        z_2017, forestArea_2017_ha, theta \\\n",
    "        = load_site_data(site_num,norm_fac=norm_fac)\n",
    "\n",
    "\n",
    "    # Evaluate Gamma values ()\n",
    "    gamma_1_vals  = gamma -  gammaSD\n",
    "    gamma_2_vals  = gamma +  gammaSD\n",
    "    gamma_size    = gamma.size\n",
    "\n",
    "    # Evaluate mean and covariances from site data\n",
    "    site_stdev       = gammaSD\n",
    "    site_covariances = np.diag(np.power(site_stdev, 2))\n",
    "    site_precisions  = np.linalg.inv(site_covariances)\n",
    "    site_mean        = gamma_1_vals/2 + gamma_2_vals/2\n",
    "\n",
    "    # Retrieve z data for selected site(s)\n",
    "    site_z_vals  = z_2017\n",
    "\n",
    "    # Initialize Gamma Values\n",
    "    gamma_vals      = gamma.copy()\n",
    "    gamma_vals_mean = gamma.copy()\n",
    "    gamma_vals_old  = gamma.copy()\n",
    "\n",
    "    # Theta Values\n",
    "    theta_vals  = theta\n",
    "\n",
    "    # Householder to track sampled gamma values\n",
    "    # gamma_vals_tracker       = np.empty((gamma_vals.size, sample_size+1))\n",
    "    # gamma_vals_tracker[:, 0] = gamma_vals.copy()\n",
    "    gamma_vals_tracker = [gamma_vals.copy()]\n",
    "\n",
    "    # Collected Ensembles over all iterations; dictionary indexed by iteration number\n",
    "    collected_ensembles = {}\n",
    "\n",
    "    # Track error over iterations\n",
    "    error_tracker = []\n",
    "\n",
    "    # Update this parameter (leng) once figured out where it is coming from\n",
    "    leng = 200\n",
    "    arr  = np.cumsum(\n",
    "             np.triu(\n",
    "             np.ones((leng, leng))\n",
    "         ),\n",
    "         axis=1,\n",
    "    ).T\n",
    "    Bdym         = (1-alpha) ** (arr-1)\n",
    "    Bdym[Bdym>1] = 0.0\n",
    "    Adym         = np.arange(1, leng+1)\n",
    "    alpha_p_Adym = np.power(1-alpha, Adym)\n",
    "\n",
    "    # Initialize Blocks of the A matrix those won't change\n",
    "    A  = np.zeros((gamma_size+2, gamma_size+2))\n",
    "    Ax = np.zeros(gamma_size+2)\n",
    "\n",
    "    # Construct Matrix B\n",
    "    B = np.eye(N=gamma_size+2, M=gamma_size, k=0)\n",
    "    B = casadi.sparsify(B)\n",
    "\n",
    "    # Construct Matrxi D constant blocks\n",
    "    D  = np.zeros((gamma_size+2, gamma_size))\n",
    "\n",
    "    # time step!\n",
    "    dt = T / N\n",
    "\n",
    "    # Other placeholders!\n",
    "    ds_vect = np.exp(- delta_t * np.arange(N+1) * dt)\n",
    "    ds_vect = np.reshape(ds_vect, (ds_vect.size, 1))\n",
    "\n",
    "    # Results dictionary\n",
    "    results = dict(\n",
    "        gamma_size=gamma_size,\n",
    "        tol=tol,\n",
    "        T=T,\n",
    "        N=N,\n",
    "        norm_fac=norm_fac,\n",
    "        delta_t=delta_t,\n",
    "        alpha=alpha,\n",
    "        kappa=kappa,\n",
    "        pf=pf,\n",
    "        pa=pa,\n",
    "        xi=xi,\n",
    "        zeta=zeta,\n",
    "        sample_size=sample_size,\n",
    "        final_sample_size=final_sample_size,\n",
    "        mode_as_solution=mode_as_solution,\n",
    "    )\n",
    "\n",
    "    # Initialize error & iteration counter\n",
    "    error = np.infty\n",
    "    cntr = 0\n",
    "\n",
    "    # Loop until convergence\n",
    "    while cntr < max_iter and error > tol:\n",
    "\n",
    "        # Update x0\n",
    "        x0_vals = gamma_vals * forestArea_2017_ha / norm_fac\n",
    "\n",
    "        # Construct Matrix A from new gamma_vals\n",
    "        A[: -2, :]        = 0.0\n",
    "        Ax[0: gamma_size] = - alpha * gamma_vals[0: gamma_size]\n",
    "        Ax[-1]            = alpha * np.sum(gamma_vals * zbar_2017)\n",
    "        Ax[-2]            = - alpha\n",
    "        A[-2, :]          = Ax\n",
    "        A[-1, :]          = 0.0\n",
    "        A = casadi.sparsify(A)\n",
    "\n",
    "        # Construct Matrix D from new gamma_vals\n",
    "        D[:, :]  = 0.0\n",
    "        D[-2, :] = -gamma_vals\n",
    "        D = casadi.sparsify(D)\n",
    "        \n",
    "        # Define the right hand side (symbolic here) as a function of gamma\n",
    "        gamma = casadi.MX.sym('gamma' , gamma_size+2)\n",
    "        up    = casadi.MX.sym('up', gamma_size)\n",
    "        um    = casadi.MX.sym('um', gamma_size)\n",
    "\n",
    "        rhs = (A @ gamma + B @ (up-um) + D @ up) * dt + gamma\n",
    "        f = casadi.Function('f', [gamma, um, up], [rhs])\n",
    "        \n",
    "\n",
    "        ## Define an optimizer and initialize it, and set constraints\n",
    "        opti = casadi.Opti()\n",
    "\n",
    "        # Decision variables for states\n",
    "        X = opti.variable(gamma_size+2, N+1)\n",
    "\n",
    "        # Aliases for states\n",
    "        Up = opti.variable(gamma_size, N)\n",
    "        Um = opti.variable(gamma_size, N)\n",
    "        Ua = opti.variable(1, N)\n",
    "\n",
    "        # 1.2: Parameter for initial state\n",
    "        ic = opti.parameter(gamma_size+2)\n",
    "\n",
    "        # Gap-closing shooting constraints\n",
    "        for k in range(N):\n",
    "            opti.subject_to(X[:, k+1] == f(X[:, k], Um[:, k], Up[:, k]))\n",
    "\n",
    "        # Initial and terminal constraints\n",
    "        opti.subject_to(X[:, 0] == ic)\n",
    "        opti.subject_to(opti.bounded(0,\n",
    "                                     X[0: gamma_size, :],\n",
    "                                     zbar_2017[0: gamma_size]\n",
    "                                     )\n",
    "                        )\n",
    "\n",
    "        # Objective: regularization of controls\n",
    "        for k in range(gamma_size):\n",
    "            opti.subject_to(opti.bounded(0, Um[k,:], casadi.inf))\n",
    "            opti.subject_to(opti.bounded(0, Up[k,:], casadi.inf))\n",
    "\n",
    "        opti.subject_to(Ua == casadi.sum1(Up+Um)**2)\n",
    "\n",
    "        # Set teh optimization problem\n",
    "        term1 = casadi.sum2(ds_vect[0: N, :].T * Ua * zeta / 2) \n",
    "        term2 = - casadi.sum2(ds_vect[0: N, :].T * (pf * (X[-2, 1: ] - X[-2, 0 :-1])))\n",
    "        term3 = - casadi.sum2(ds_vect.T * casadi.sum1( (pa * theta_vals - pf * kappa ) * X[0: gamma_size, :] ))\n",
    "        \n",
    "        opti.minimize(term1 + term2 + term3)\n",
    "\n",
    "        # Solve optimization problem\n",
    "        options               = dict()\n",
    "        options[\"print_time\"] = False\n",
    "        options[\"expand\"]     = True\n",
    "        options[\"ipopt\"]      = {'print_level':                      0,\n",
    "                                 'fast_step_computation':            'yes',\n",
    "                                 'mu_allow_fast_monotone_decrease':  'yes',\n",
    "                                 'warm_start_init_point':            'yes',\n",
    "                                 }\n",
    "        opti.solver('ipopt', options)\n",
    "        \n",
    "        opti.set_value(ic,\n",
    "                       casadi.vertcat(site_z_vals,\n",
    "                                      np.sum(x0_vals),\n",
    "                                      1),\n",
    "                       )\n",
    "        \n",
    "        if _DEBUG:\n",
    "            print(\"ic: \", ic)\n",
    "            print(\"site_z_vals: \", site_z_vals)\n",
    "            print(\"x0_vals: \", x0_vals)\n",
    "            print(\"casadi.vertcat(site_z_vals,np.sum(x0_vals),1): \", casadi.vertcat(site_z_vals,np.sum(x0_vals),1))\n",
    "        sol = opti.solve()\n",
    "\n",
    "        if _DEBUG:\n",
    "            print(\"sol.value(X)\", sol.value(X))\n",
    "            print(\"sol.value(Ua)\", sol.value(Ua))\n",
    "            print(\"sol.value(Up)\", sol.value(Up))\n",
    "            print(\"sol.value(Um)\", sol.value(Um))\n",
    "        \n",
    "        \n",
    "        ## Start Sampling\n",
    "        # Update signature of log density evaluator\n",
    "        log_density = lambda gamma_val: log_density_function(gamma_val=gamma_val,\n",
    "                                                             gamma_vals_mean=gamma_vals_mean,\n",
    "                                                             theta_vals=theta_vals,\n",
    "                                                             site_precisions=site_precisions,\n",
    "                                                             alpha=alpha,\n",
    "                                                             sol=sol,\n",
    "                                                             X=X,\n",
    "                                                             Ua=Ua,\n",
    "                                                             Up=Up,\n",
    "                                                             zbar_2017=zbar_2017,\n",
    "                                                             forestArea_2017_ha=forestArea_2017_ha,\n",
    "                                                             norm_fac=norm_fac,\n",
    "                                                             alpha_p_Adym=alpha_p_Adym,\n",
    "                                                             Bdym=Bdym,\n",
    "                                                             leng=leng,\n",
    "                                                             T=T,\n",
    "                                                             ds_vect=ds_vect,\n",
    "                                                             zeta=zeta,\n",
    "                                                             xi=xi,\n",
    "                                                             kappa=kappa,\n",
    "                                                             pa=pa,\n",
    "                                                             pf=pf,\n",
    "                                                             )\n",
    "\n",
    "        # Create MCMC sampler & sample, then calculate diagnostics\n",
    "        sampler = create_hmc_sampler(\n",
    "            size=gamma_size,\n",
    "            log_density=log_density,\n",
    "            #\n",
    "            burn_in=100,\n",
    "            mix_in=2,\n",
    "            symplectic_integrator='verlet',\n",
    "            symplectic_integrator_stepsize=1e-1,\n",
    "            symplectic_integrator_num_steps=3,\n",
    "            mass_matrix=1e+1,\n",
    "            constraint_test=lambda x: True if np.all(x>=0) else False,\n",
    "        )\n",
    "        gamma_post_samples = sampler.sample(\n",
    "            sample_size=sample_size,\n",
    "            initial_state=gamma_vals,\n",
    "            verbose=True,\n",
    "        )\n",
    "        gamma_post_samples = np.asarray(gamma_post_samples)\n",
    "\n",
    "        # Update ensemble/tracker\n",
    "        collected_ensembles.update({cntr: gamma_post_samples.copy()})\n",
    "\n",
    "        # Update gamma value\n",
    "        weight     = 0.25  # <-- Not sure how this linear combination weighting helps!\n",
    "        if mode_as_solution:\n",
    "            raise NotImplementedError(\"We will consider this in the future; trace sampled points and keep track of objective values to pick one with highest prob. \")\n",
    "            \n",
    "        else:\n",
    "            gamma_vals = weight * np.mean(gamma_post_samples, axis=0 ) + (1-weight) * gamma_vals_old\n",
    "        gamma_vals_tracker.append(gamma_vals.copy())\n",
    "\n",
    "        # Evaluate error for convergence check\n",
    "        error = np.max(np.abs(gamma_vals_old-gamma_vals) / gamma_vals_old)\n",
    "        error_tracker.append(error)\n",
    "        print(f\"Iteration [{cntr+1:4d}]: Error = {error}\")\n",
    "\n",
    "        # Exchange gamma values (for future weighting/update & error evaluation)\n",
    "        gamma_vals_old = gamma_vals\n",
    "\n",
    "        # Increase the counter\n",
    "        cntr += 1\n",
    "\n",
    "        results.update({'cntr': cntr,\n",
    "                        'error_tracker':np.asarray(error_tracker),\n",
    "                        'gamma_vals_tracker': np.asarray(gamma_vals_tracker),\n",
    "                        'collected_ensembles':collected_ensembles,\n",
    "                        })\n",
    "        pickle.dump(results, open('results.pcl', 'wb'))\n",
    "        \n",
    "        # Extensive plotting for monitoring; not needed really!\n",
    "        if False:\n",
    "            plt.plot(gamma_vals_tracker[-2], label=r'Old $\\gamma$')\n",
    "            plt.plot(gamma_vals_tracker[-1], label=r'New $\\gamma$')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            for j in range(gamma_size):\n",
    "                plt.hist(gamma_post_samples[:, j], bins=50)\n",
    "                plt.title(f\"Iteration {cntr}; Site {j+1}\")\n",
    "                plt.show()\n",
    "    \n",
    "    print(\"Terminated. Sampling the final distribution\")\n",
    "    # Sample (densly) the final distribution\n",
    "    final_sample = sampler.sample(\n",
    "        sample_size=final_sample_size,\n",
    "        initial_state=gamma_vals,\n",
    "        verbose=True,\n",
    "    )\n",
    "    final_sample = np.asarray(final_sample)\n",
    "    results.update({'final_sample': final_sample})\n",
    "    pickle.dump(results, open('results.pcl', 'wb'))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2566dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "\n",
      "=====================================================\n",
      "Started Sampling\n",
      "=====================================================\n",
      "\n",
      "HMC Iteration [ 536/2100]; Accept Prob: 1.00; --> Accepted? True  "
     ]
    }
   ],
   "source": [
    "results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c79f95",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5016be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Error Results\n",
    "plt.plot(results['error_tracker'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Gamma Estimate Update\n",
    "for j in range(results['gamma_size']):\n",
    "    plt.plot(results['gamma_vals_tracker'][:, j], label=r\"$\\gamma_{%d}$\"%(j+1))\n",
    "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fbf2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histograms\n",
    "for itr in results['collected_ensembles'].keys():\n",
    "    for j in range(results['gamma_size']):\n",
    "        plt.hist(results['collected_ensembles'][itr][:, j], bins=100)\n",
    "        plt.title(f\"Iteration {itr+1}; Site {j+1}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histogram of the final sample\n",
    "for j in range(results['gamma_size']):\n",
    "    plt.hist(results['final_sample'][:, j], bins=100)\n",
    "    plt.title(f\"Final Sample; Site {j+1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3be0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7eb6cc-d49c-417a-bd2b-2d59b6438fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0944f72-8772-4c3c-b271-86f953ccdcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec13734-3a20-4e3f-af7a-5d1ab1f21427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
